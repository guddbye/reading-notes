In a world where things like AI are becoming more and more integrated in our culture, it is crucial that you think carefully about the ethics of anything you are building since you are the final line of defense between the program you have been tasked with creating and the general public. If you work for a corporation and you see anything that you think is immoral, you should speak out about it because they are more inclined to listen to you than they are to public protest, and occasionally that may make a difference. You must make sure that strong technology is used for good and not harm while creating it.

The morality of the software used in self-driving automobiles is a more pressing and direct issue. We need to program exactly what the car will do, up to and including choosing who is most likely to die in every circumstance, since when a person is driving, we cannot forecast what their split-second judgment will be in a life or death situation. Companies no longer have to be as specific about what they want when it comes to other concerns like data collecting, which allows them to capture everything and decide on a use for it afterwards. This is a risky way to approach data.

Google's AI principles have been described as the requirement for social benefit, avoiding the creation or reinforcement of unfair bias, being built and tested for safety, being accountable to people, incorporating privacy design principles, upholding the highest standards of scientific excellence, and being made available for uses that are consistent with their principles. Additionally, they declare that they won't work on AI projects that might hurt society as a whole, be used as a weapon, be used for surveillance, or violate human rights. I do think it's odd that the prior report about Google working on a filtered search engine for China only surfaced a few months after this set of recommendations.
